{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "FCLB_TFOD.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonJM/FCLB-TFOD/blob/master/FCLB_TFOD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIV8j7m1jXDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e88b66-e975-415c-f20e-f706f403a34f"
      },
      "source": [
        "!pip install wget\n",
        "\n",
        "#import neccessary python modules and libraries\n",
        "import os\n",
        "import shutil\n",
        "from PIL import ImageEnhance\n",
        "from PIL import Image\n",
        "import uuid\n",
        "import tarfile\n",
        "import wget"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=e7ecc3176244c03d5a880b54bba7972416c167dbd1796f6963cd3150beed972a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78s0FVL4mMg5"
      },
      "source": [
        "# Adding Classes for Path Setup and Project Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My2TVbX8lvMq"
      },
      "source": [
        "class Setup_Path: \n",
        "\n",
        "    LABELS = [\"fresh\", \"rotten\"]\n",
        "\n",
        "    def getFiles(self):\n",
        "      files = {\n",
        "          \"LABELMAP\":os.path.join(self.getWorkspaces()['ANNOTATIONS_PATH'], \"label_map.pbtxt\"),\n",
        "          'TFRECORD_SCRIPT':os.path.join(self.getWorkspaces()['PREPROCESSING_PATH'], 'generate_tfrecord.py')\n",
        "      }\n",
        "\n",
        "      return files\n",
        "\n",
        "    def getWorkspaces(self):\n",
        "        paths = {\n",
        "            \"ADDONS_PATH\":os.path.join(\"Tensorflow\", \"addons\"),\n",
        "            \"LABEL_IMG_PATH\":os.path.join(\"Tensorflow\", \"addons\", \"labelImg\"),\n",
        "            \"TFOD_API_PATH\":\"Tensorflow\",\n",
        "            \"WORKSPACE_PATH\":os.path.join(\"Tensorflow\", \"workspace\"),\n",
        "            \"TRAINING_DEMO_PATH\":os.path.join(\"Tensorflow\", \"workspace\", \"training_demo\"),\n",
        "            \"ANNOTATIONS_PATH\":os.path.join(\"Tensorflow\", \"workspace\", \"training_demo\", \"annotations\"),\n",
        "            \"EXPORTED_MODELS_PATH\":os.path.join(\"Tensorflow\", \"workspace\", \"training_demo\", \"exported-models\"),\n",
        "            \"IMAGES_PATH\":os.path.join(\"Tensorflow\", \"workspace\", \"training_demo\", \"images\"),\n",
        "            \"TRAINING_IMAGES_PATH\":os.path.join(\"Tensorflow\", \"workspace\", \"training_demo\", \"images\", \"train\"),\n",
        "            \"TESTING_IMAGES_PATH\":os.path.join(\"Tensorflow\", \"workspace\", \"training_demo\", \"images\", \"test\"),\n",
        "            \"MODELS_PATH\":os.path.join(\"Tensorflow\", \"workspace\", \"training_demo\"),\n",
        "            \"PRETRAINED_MODELS_PATH\":os.path.join(\"Tensorflow\", \"workspace\", \"training_demo\", \"pre-trained-models\"),\n",
        "            \"PREPROCESSED_IMG\":os.path.join(\"Tensorflow\", \"addons\", \"preprocessedImg\"),\n",
        "            \"ORIGINAL_IMGS\":os.path.join(\"Tensorflow\", \"addons\", \"originalImg\"),   \n",
        "            \"PREPROCESSING_PATH\":os.path.join(\"Tensorflow\", \"scripts\", \"preprocessing\"),\n",
        "            \"PROTOS_PATH\":os.path.join(\"Tensorflow\", \"Google-Protobuf\"),\n",
        "        }\n",
        "\n",
        "        return paths\n",
        "\n",
        "    # def set_main_paths(self):\n",
        "    #     for path in self.get_main_paths().values():\n",
        "    #         if not os.path.exists(path):\n",
        "    #             os.makedirs(path)\n",
        "\n",
        "    # def get_image_paths(self):\n",
        "    #     path = {\n",
        "    #         \"ORIGINAL_IMAGES_PATH\" : [\"workspace\\datasets\\original_images\\Fresh_Ripe\", \"workspace\\datasets\\original_images\\Fresh_Unripe\", \"workspace\\datasets\\original_images\\Rotten_Ripe\", \"workspace\\datasets\\original_images\\Rotten_Unripe\"],\n",
        "    #         \"PREPROCESSED_IMAGES_PATH\" : [\"workspace\\datasets\\preprocessed_images\\Fresh_Ripe\", \"workspace\\datasets\\preprocessed_images\\Fresh_Unripe\", \"workspace\\datasets\\preprocessed_images\\Rotten_Ripe\", \"workspace\\datasets\\preprocessed_images\\Rotten_Unripe\"],\n",
        "    #     }\n",
        "\n",
        "    #     return path\n",
        "\n",
        "    # def set_image_paths(self):\n",
        "    #     for path in self.get_image_paths().values():\n",
        "    #         for label in path:\n",
        "    #             if not os.path.exists(label):\n",
        "    #                 os.makedirs(label)\n",
        "\n",
        "    # def getFiles(self):\n",
        "    #     files = {\n",
        "    #         'PIPELINE_CONFIG':os.path.join(self.get_paths()['MODELS_PATH'], self.custom_name, 'pipeline.config'),\n",
        "    #         'TF_RECORD_SCRIPT':os.path.join(self.get_paths()['TFRECORD_GENERATOR_PATH'], self.tfrecord_name),\n",
        "    #         'LABEL_MAP':os.path.join(self.get_paths()['ANOTATIONS_PATH'], self.map_name),\n",
        "    #     }\n",
        "    #     return files"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lD26arHmG7-"
      },
      "source": [
        "class FCLB_Datasets:\n",
        "    #class' constructor\n",
        "    def __init__(self, setup):\n",
        "        self.setup = setup\n",
        "    \n",
        "    #acquiring uncategorized dataset\n",
        "    def get_original_dataset(self, num1, num2, label):\n",
        "        images_path = self.setup.getWorkspaces()['ORIGINAL_IMGS']\n",
        "        path = os.path.join(images_path, label)\n",
        "\n",
        "        files = [\"{}\".format(i) for i in os.listdir(path)[num1:num2]]\n",
        "\n",
        "        return files\n",
        "\n",
        "    def copy_image_file(self, num1, num2, srcPath, dstPath):\n",
        "\n",
        "        for label in self.setup.LABELS:\n",
        "            for file in self.get_original_dataset(num1, num2, label):\n",
        "                dst = os.path.join(dstPath, label, file)\n",
        "                src = os.path.join(srcPath, label, file)\n",
        "                shutil.copyfile(src, dst)\n",
        "\n",
        "    #I call it segment function, wala na koy mahunahunaan\n",
        "    def segment(self):\n",
        "        original_dir = self.setup.getWorkspaces()['ORIGINAL_IMGS']\n",
        "        target_dir = self.setup.getWorkspaces()['PREPROCESSED_IMG']\n",
        "        labels = self.setup.LABELS\n",
        "\n",
        "        for label in labels:\n",
        "            original = os.path.join(original_dir, label)\n",
        "            target = os.path.join(target_dir, label)\n",
        "            for img in os.listdir(original):\n",
        "                self.segmentation(original, target, img, label)\n",
        " \n",
        "\n",
        "    #this function used to segment an image file, it actually resize image, add contrast and lightness\n",
        "    #and save it to the target directory\n",
        "    #the purpose of uuid module is to give unique id or name to an image file\n",
        "    def segmentation(self, base_dir, target_dir, fname, label):\n",
        "        image = Image.open(os.path.join(base_dir, fname))\n",
        "        size = (400,400)\n",
        "        image.thumbnail(size)\n",
        "\n",
        "        contrast = ImageEnhance.Contrast(image)\n",
        "        contrast.enhance(1.8).save(os.path.join(target_dir, label + \"-\"+ '{}.jpg'.format(uuid.uuid1())))\n",
        "\n",
        "        brightness = ImageEnhance.Brightness(image)\n",
        "        brightness.enhance(1.2).save(os.path.join(target_dir, label +\"-\"+ '{}.jpg'.format(uuid.uuid1())))\n",
        "\n",
        "    #compressed file for github upload\n",
        "    def compressed(self):\n",
        "        path = self.getDir('PREPROCESSED_IMAGES_PATH')\n",
        "        sub_paths = self.getSubDirectories(path)\n",
        "        ARCHIVE_PATH = os.path.join(path, 'preprocessed_images.tar.gz')\n",
        "        tar = tarfile.open(ARCHIVE_PATH, \"w:gz\")\n",
        "        for sub_path in sub_paths:\n",
        "            dir = os.path.join(path, sub_path)\n",
        "            tar.add(dir)\n",
        "        tar.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC41kBtRm0Yp"
      },
      "source": [
        "# Preparing Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ9E39oJvC4L"
      },
      "source": [
        "Creating a workspace directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxX3KjTwm38H"
      },
      "source": [
        "setup = Setup_Path()\n",
        "workspaces = setup.getWorkspaces()\n",
        "for workspace in workspaces.values():\n",
        "  if not os.path.exists(workspace):\n",
        "    os.makedirs(workspace)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4kFQ6HvBPLs"
      },
      "source": [
        "Create label directory for original images and preprocessed images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E57aJQJaBZwk"
      },
      "source": [
        "for label in setup.LABELS:\n",
        "  preprocessedImg_labelPath = os.path.join(setup.getWorkspaces()['PREPROCESSED_IMG'], label)\n",
        "  if not os.path.exists(preprocessedImg_labelPath):\n",
        "    os.makedirs(preprocessedImg_labelPath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V3MqcrhCw31"
      },
      "source": [
        "segment images and copy to preprocessed directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTt-b7Kf4i8w"
      },
      "source": [
        "dataset = FCLB_Datasets(setup)\n",
        "dataset.segment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGxfLOMGHcVz"
      },
      "source": [
        "# Image Labelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6s2JNoIHayP",
        "outputId": "4da497a5-7042-470b-d3ef-1f7c5b93bd12"
      },
      "source": [
        "labelImg = setup.getWorkspaces()['LABEL_IMG_PATH']\n",
        "!cd {labelImg} && git clone https://github.com/tzutalin/labelImg.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'labelImg'...\n",
            "remote: Enumerating objects: 1846, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 1846 (delta 37), reused 55 (delta 18), pack-reused 1759\u001b[K\n",
            "Receiving objects: 100% (1846/1846), 232.81 MiB | 41.39 MiB/s, done.\n",
            "Resolving deltas: 100% (1084/1084), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_mc2redH-cK",
        "outputId": "bc071297-da4d-47f4-ae0b-be68a15cee4a"
      },
      "source": [
        "labelImg = os.path.join(setup.getWorkspaces()['LABEL_IMG_PATH'], \"labelImg\")\n",
        "!cd {labelImg} && pip install pyqt5\n",
        "!cd {labelImg} && pyrcc5 -o libs/resources.py resources.qrc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyqt5 in /usr/local/lib/python3.7/dist-packages (5.15.4)\n",
            "Requirement already satisfied: PyQt5-sip<13,>=12.8 in /usr/local/lib/python3.7/dist-packages (from pyqt5) (12.9.0)\n",
            "Requirement already satisfied: PyQt5-Qt5>=5.15 in /usr/local/lib/python3.7/dist-packages (from pyqt5) (5.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aapLubxmPV5f"
      },
      "source": [
        "# Partition Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kY92xAFPs7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d3d8d811-82e1-4a65-c447-a9f08494c89f"
      },
      "source": [
        "url = \"https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/_downloads/d0e545609c5f7f49f39abc7b6a38cec3/partition_dataset.py\"\n",
        "src = wget.download(url)\n",
        "dst = setup.getWorkspaces()['PREPROCESSING_PATH']\n",
        "shutil.move(src, dst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tensorflow/scripts/preprocessing/partition_dataset.py'"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nnlfqUdacOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71e4808-c478-4f46-9dc5-64cce3c4060e"
      },
      "source": [
        "images_path = setup.getWorkspaces()['IMAGES_PATH']\n",
        "!cd {dst} python partition_dataset.py -x -i {images_path} -r 0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: too many arguments\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNwjmYxpjAsE"
      },
      "source": [
        "# Download Tensorflow Object Detection API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLUHiHepje2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0025c2ae-cb7f-42a4-9740-fbc5d17afaed"
      },
      "source": [
        "tfod_api = setup.getWorkspaces()['TFOD_API_PATH']\n",
        "!cd {tfod_api} && git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 61038, done.\u001b[K\n",
            "remote: Total 61038 (delta 0), reused 0 (delta 0), pack-reused 61038\u001b[K\n",
            "Receiving objects: 100% (61038/61038), 573.94 MiB | 31.68 MiB/s, done.\n",
            "Resolving deltas: 100% (42513/42513), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCU3DQZ6k5EY"
      },
      "source": [
        "Install Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgVwgIwDlmB6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "33f14220-bd12-4b89-b2cf-e8ca10d92b3f"
      },
      "source": [
        "url=\"https://github.com/protocolbuffers/protobuf/releases/download/v3.15.6/protoc-3.15.6-win64.zip\"\n",
        "src = wget.download(url)\n",
        "protos_path = setup.getWorkspaces()['PROTOS_PATH']\n",
        "shutil.move(src, protos_path)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-9456d3985292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprotos_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetWorkspaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PROTOS_PATH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotos_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd {protos_path} && unzip {src}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PATH'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpathsep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotos_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mreal_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Destination path 'Tensorflow/Google-Protobuf/protoc-3.15.6-win64.zip' already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh_rvea2eayt",
        "outputId": "754b1cdc-d879-4739-f8f2-ba8d9bfca469",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "os.environ['PATH'] += os.pathsep + os.path.abspath(os.path.join(protos_path, 'bin'))   \n",
        "!cd {research_path} && protoc object_detection/protos/*.proto --python_out=. && cp object_detection\\\\packages\\\\tf2\\\\setup.py setup.py && python setup.py build && python setup.py install\n",
        "!cd {slim_path} && pip install -e . "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'object_detection\\packages\\tf2\\setup.py': No such file or directory\n",
            "Obtaining file:///content/Tensorflow/models/research/slim\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from slim==0.1) (1.15.0)\n",
            "Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.7/dist-packages (from slim==0.1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim>=1.1->slim==0.1) (0.12.0)\n",
            "Installing collected packages: slim\n",
            "  Attempting uninstall: slim\n",
            "    Found existing installation: slim 0.1\n",
            "    Can't uninstall 'slim'. No files were found to uninstall.\n",
            "  Running setup.py develop for slim\n",
            "Successfully installed slim-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5SWZ2MPaN2-",
        "outputId": "34268b55-3b4e-4bdb-82b2-2f4830df3a6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !cd {research_path} && python object_detection/builders/model_builder_tf2_test.py\n",
        "VERIFICATION_SCRIPT = os.path.join(research_path, 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
        "# Verify Installation\n",
        "!python {VERIFICATION_SCRIPT}"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"Tensorflow/models/research/object_detection/builders/model_builder_tf2_test.py\", line 25, in <module>\n",
            "    from object_detection.builders import model_builder\n",
            "ModuleNotFoundError: No module named 'object_detection'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhRsKANibnO4"
      },
      "source": [
        "# Create Label Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgtI37vKbpoH"
      },
      "source": [
        "labels = [{'name':'fresh', 'id':1}, {'name':'rotten', 'id':2}]\n",
        "\n",
        "with open(setup.getFiles()['LABELMAP'], 'w') as f:\n",
        "    for label in labels:\n",
        "        f.write('item { \\n')\n",
        "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
        "        f.write('\\tid:{}\\n'.format(label['id']))\n",
        "        f.write('}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lyEfFJfgRCZ"
      },
      "source": [
        "# Create Tensorflow Records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JW09lWrgTBN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "221b9f1b-6a31-4f85-b7c5-8d75c6f84c11"
      },
      "source": [
        "url = 'https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/_downloads/da4babe668a8afb093cc7776d7e630f3/generate_tfrecord.py'\n",
        "src = wget.download(url)\n",
        "dst = setup.getWorkspaces()['PREPROCESSING_PATH']\n",
        "shutil.move(src, dst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tensorflow/scripts/preprocessing/generate_tfrecord.py'"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkhzhSrlgiPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc23324-1e34-4814-f1ee-b0fa2933ca43"
      },
      "source": [
        "# Create train data:\n",
        "!python {setup.getFiles()['TFRECORD_SCRIPT']} -x {setup.getWorkspaces()['TRAINING_IMAGES_PATH']} -l {setup.getFiles()['LABELMAP']} -o {os.path.join(setup.getWorkspaces()['ANNOTATIONS_PATH'], 'train.record')}\n",
        "\n",
        "# Create test data:\n",
        "!python {setup.getFiles()['TFRECORD_SCRIPT']} -x {setup.getWorkspaces()['TESTING_IMAGES_PATH']} -l {setup.getFiles()['LABELMAP']} -o {os.path.join(setup.getWorkspaces()['ANNOTATIONS_PATH'], 'train.record')}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"Tensorflow/scripts/preprocessing/generate_tfrecord.py\", line 29, in <module>\n",
            "    from object_detection.utils import dataset_util, label_map_util\n",
            "ModuleNotFoundError: No module named 'object_detection'\n",
            "Traceback (most recent call last):\n",
            "  File \"Tensorflow/scripts/preprocessing/generate_tfrecord.py\", line 29, in <module>\n",
            "    from object_detection.utils import dataset_util, label_map_util\n",
            "ModuleNotFoundError: No module named 'object_detection'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}